# Desafio Sprint 8


## üìã **Descri√ß√£o do desafio e o meu passo a passo**  
Objetivo: Usar o Apache Spark, atrav√©s do AWS Glue para integrar e padronizar dados existentes na camada Raw Zone para a Trusted Zone. 


## Inicialmente eu comecei criando um job para refinar os dados do CSV, filtrando por crit√©rios espec√≠ficos e salvando no formato Parquet na camada Trusted

### Logo abaixo est√£o as configura√ß√µes desse job, elas foram ajustadas de acordo com o que foi pedido na documenta√ß√£o. Decidi chamar o job de "CSV_Raw_Para_Trusted"

![configura√ß√µes job csv](../evidencias/print_desafio/job_details_csv.png)

![configura√ß√µes job csv 2](../evidencias/print_desafio/job_details_csv_2.png)

### Logo em seguida constru√≠ o meu c√≥digo no job

### Em resumo ele carrega um arquivo CSV do meu bucket no S3 que est√° na Raw Zone para um DynamicFrame

### Depois disso converte o DynamicFrame para DataFrame para fazer as seguintes tarefas:

### Remover linhas duplicatas, substituir valores inv√°lidos (\N) por None e filtrar registros cujo campo "g√™nero" cont√©m "Crime" ou "Guerra"

### Por fim, converte o DataFrame processado de volta para DynamicFrame e grava no formato Parquet no bucket S3 na Trusted Zone

### O script logo abaixo

![c√≥digo script CSV](../evidencias/print_desafio/script_csv.png)

## Agora um pouco sobre o segundo job que serve para refinar os dados JSON da camada Raw, realizando limpeza e normaliza√ß√£o, e salvar no formato Parquet na camada Trusted

### Primeiro, as configura√ß√µes do job, ele se chama "JSON_Raw_Para_Trusted"

![configura√ß√µes job json](../evidencias/print_desafio/job_details_json.png)

![configura√ß√µes job json 2](../evidencias/print_desafio/job_details_json_2.png)

### Agora um pouco sobre o c√≥digo

### Ele carrega arquivos JSON da camada Raw no S3 em um DynamicFrame

### Depois converte o DynamicFrame em DataFrame para fazer as seguintes fun√ß√µes:

### Remover registros duplicados, converter a coluna "Data de Lan√ßamento" para o formato de data (yyyy-MM-dd) e substituir valores "0" nas colunas "Or√ßamento" e "Receita" por None

### No final, converte o DataFrame processado de volta para DynamicFrame e salva no formato Parquet na Trusted Zone no S3

### Script com o c√≥digo abaixo

![conte√∫do script json](../evidencias/print_desafio/script_json.png)

## Depois de finalizar os jobs, executei ambos

![execu√ß√£o job csv](../evidencias/print_desafio/executando_CSV.png)

![execu√ß√£o job json](../evidencias/print_desafio/executando_json.png)

## O arquivo parquet gerado a partir do CSV, na documenta√ß√£o do desafio n√£o pediu particionamento

![csv no bucket](../evidencias/print_desafio/csv_bucket.png)

## Agora o arquivo parquet gerado do JSON, o particionamento de acordo com a data que os dados foram retirados do TMDB como pedido no desafio

![json no bucket](../evidencias/print_desafio/json_bucket.png)


